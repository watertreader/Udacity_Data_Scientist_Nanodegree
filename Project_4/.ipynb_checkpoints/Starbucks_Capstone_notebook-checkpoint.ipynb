{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starbucks Capstone Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "For this project, we will have an opportunity to analyse Starbucks mobile app customer base data and their responses towards promotion offers. First we will explore, clean the data and then make exploratory analysis to find out customer based statistic. Subseqently, we aim to make on what the app should make recommendation offer to the new customer on the app with a machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Starbucks and Mobile App Program\n",
    "\n",
    "### Starbuck\n",
    "Starbucks Corporation is an American multinational chain of coffeehouses and roastery reserves headquartered in Seattle, Washington since 1971[1]. It is currently the world's largest coffeehouse chain store, serving a variety of beverages from hot/cold coffee/tea to the In addition to drinks and food, many stores carry Starbucks' official merchandise, such as mugs, tumblers, scoops, and coffee presses. Not limited to store front sales, drinks (canned drink, expresso capsules) sporting Starbuck label has appeared in supermarket and online shops[2]\n",
    "\n",
    "### Mobile Application\n",
    "Starbucks is one of the early adopter of mobile application. In May 2008, a loyalty program was introduced for Starbucks Card registered users offering perks such as free Wi-Fi Internet access etc. 3 Years on, it began beta testing its mobile app for the Starbucks card, offering prepaid services. Subsequently, Starbucks released its complete mobile platform by January 2011. By December 2011, the number of mobile transactions exceeded 26 million. It is estimated that 30% of the Starbuck transactions is through mobile compared to Apple Pay, which is only used by ~5% of customers in 2018 [3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information on Data Set\n",
    "The provided data set for this project contains simulated data (containing transactions, customer spread and offer types) which mimics customer behavior on the Starbucks rewards mobile app. Once every few days, Starbucks sends out an offer to users of the mobile app. An offer can be merely an advertisement for a drink or an actual offer such as a discount or BOGO (buy one get one free). Some users might not receive any offer during certain weeks and not every user will have the same offer offered to the next\n",
    "\n",
    "#### Validity Period\n",
    "Every offer has a validity period before the offer expires. For example, a BOGO offer might be valid for only 5 days. The data set contains also informational offers which are merely providing information about a product. Accordingly, if an informational offer has 7 days of validity, one can assumed the customer is under the influence of the offer for 7 days after receiving the advertisement. It can be shown from the transactional data shows the timestamp of each purchase as well as the amount of money spent when user make purchases on the app. The transactional data also has a record for each offer that a user receives, a record for when a user actually views the offer and finally when a user completes an offer. \n",
    "\n",
    "Conversely, user should also note that it is possible that a customer using the app might make a purchase through the app without having received an offer or seen an offer.\n",
    "\n",
    "#### Rewards System\n",
    "To give an example how the reward process works, a user could receive a discount offer buy 10 dollars get 2 off. The offer is valid for 10 days from receipt. If the customer accumulates at least 10 dollars in purchases during the validity period, the customer completes the offer. \n",
    "\n",
    "It has also to be considered that a customer can receive an offer, never actually views the offer, and still completes the offer. For example, a customer might receive the \"buy 10 dollars get 2 dollars off offer\", but never opens the offer during the 10 day validity period, while he spends 15 dollars during those ten days. Thus there will be an offer completion record in the data set while the customer was not influenced by the offer.[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Data Sets\n",
    "\n",
    "The data is contained in three files:\n",
    "\n",
    "* portfolio.json - containing offer ids and meta data about each offer (duration, type, etc.)\n",
    "* profile.json - demographic data for each customer\n",
    "* transcript.json - records for transactions, offers received, offers viewed, and offers completed\n",
    "\n",
    "**Note:** If you are using the workspace, you will need to go to the terminal and run the command `conda update pandas` before reading in the files. This is because the version of pandas in the workspace cannot read in the transcript.json file correctly, but the newest version of pandas can. You can access the termnal from the orange icon in the top left of this notebook.  \n",
    "\n",
    "You can see how to access the terminal and how the install works using the two images below.  First you need to access the terminal:\n",
    "\n",
    "<img src=\"pic1.png\"/>\n",
    "\n",
    "Then you will want to run the above command:\n",
    "\n",
    "<img src=\"pic2.png\"/>\n",
    "\n",
    "Finally, when you enter back into the notebook (use the jupyter icon again), you should be able to run the below cell without any errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I : Define the business problem\n",
    "\n",
    "The first part of our studies is to define our business problem\n",
    "<ol>\n",
    "    <li> What is the age, gender and income demographics of the customer group? Who formed the majority of the Starbucks customer base ?</li>\n",
    "    <li> Which is the most effective channel in dispensing out information </li>\n",
    "    <li> Which group (age, income, membership length) of customers appear to be more responsive towards the promotion\n",
    "    <li> Is there link between the offer given and the target group? </li>\n",
    "    <li> Which type of promotional offer most appeal to which group of customer?</li>\n",
    "    <li>Can we build a recommendation engine to recommend promotional offer with good uptake rate to new customers based on their demographical data age, income, registration date and gender?</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# read in the json files\n",
    "portfolio   = pd.read_json('data/portfolio.json', orient='records', lines=True)\n",
    "profile     = pd.read_json('data/profile.json',   orient='records', lines=True)\n",
    "transcript  = pd.read_json('data/transcript.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## update the pandas package as suggested above\n",
    "\n",
    "# if (float(pd.__version__) > 1.53):\n",
    "#     print('version is greater')\n",
    "# !conda update pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II : Analyze\n",
    "\n",
    "\n",
    "We will analyze the problem(with the 3 pandas DataFrame) through visualizations and data exploration to have a better understanding of what algorithms and features are appropriate for solving it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "portfolio.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of Portfolio Data\n",
    "\n",
    "This data contains 10 campaign offers that is run together with Starbuck Customer, of which there are 3 types of promotion types being offered and the campaign offers is run with up to 4 types of channels. There is no missing data in the set and so we did not need to impute any value. Below tells of the detail for each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**portfolio.json schema**\n",
    "* id (string) - offer id\n",
    "* offer_type (string) - type of offer ie BOGO, discount, informational\n",
    "* difficulty (int) - minimum required spend to complete an offer, - cost to consumer\n",
    "* reward (int) - reward given for completing an offer - cost to Starbuck\n",
    "* duration (int) - time for offer to be open, in days\n",
    "* channels (list of strings), ie web, email, mobile, social"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following cleansing work would be neccessary\n",
    "<ol> \n",
    "    <li> To expand the channels column into multiple columns based on the list atttribute: namely web, email, mobile and social  \n",
    "    <li> To drop some of the channels namely channels and email. Email is used in every info campaign while channels have been expanded and is redundant\n",
    "    <li> To expand the offer_type, a form of categorical data, into individual columns\n",
    "    <li> Convert the duration to number of hours instead of days\n",
    "    <li> Normalize some of the value \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find out how many person is of age 118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.age[profile.age==118].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of people with age 118 matches with the number of people without gender and without income data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of Profile Data\n",
    "\n",
    "A glimpse at the data show there is a number of customers, (2175 to be exact) at 118 year old! A small trivial[4], the oldest known living person  has managed to live a year of 122 and the number of people who have live to that age is not more than 10. Starbucks could not possibly get so many customer who is of 118. <br>\n",
    "\n",
    "A plausible explanation could be that the customer refuse to leave his/her identity information. The number of customer who did not have their gender, income or age is the same, 2175. We might be wanting to remove this set of user, even though it represented about 10% of the total user database\n",
    "\n",
    "We might also want to convert the became member "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**profile.json schema**\n",
    "* age (int) - age of the customer \n",
    "* became_member_on (int) - date when customer created an app account\n",
    "* gender (str) - gender of the customer (note some entries contain 'O' for other rather than M or F)\n",
    "* id (str) - customer id\n",
    "* income (float) - customer's income"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cleansing work would be neccessary\n",
    "\n",
    "<ol> \n",
    "    <li> To drop the members with 118 years of age     \n",
    "    <li> To convert \"became_member_on\" to \"membership\". Would need to change from string format YYYYMMDD representatation to one that is based on year they have been member\n",
    "    <li> Normalize some of the value \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transcript.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript.event.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it must be stressed the transcript count presented here includes transaction of people with missing profile information which we will not use eventually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of Transcript Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transcript.json\n",
    "\n",
    "    event (str) - record description of type transaction, offer received, offer viewed, offer completed\n",
    "    person (str) - customer id\n",
    "    time (int) - time in hours since start of test. The data begins at time t=0\n",
    "    value - (dict of strings) - either an offer id or transaction amount depending on the record\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cleansing work would be neccessary \n",
    "\n",
    "<ol>\n",
    "    <li> To remove transaction of those users which have been removed in profile json \n",
    "    <li> To expand the event column, a form of categorical data, into individual binary columns of transaction, offer received, offer viewed, offer completed\n",
    "    <li> To make matter simple, formed a new column call campaign_success based on customer would have to view the offer and complete the offer, then it would consider as campaign success while completion of offer without actually viewing the offer would consider to be a failure\n",
    "    <li> To extract out the value type into either one indicating of offer_id or transaction amount. Consideration is also taken to drop those transaction amount data\n",
    "    <li> Normalize some of the value \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally then combine the 3 dataframe to form a master dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III :  Data Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform cleaning and engineering for all data sets (portfolio, profile, transcript). In summary, we will focus on the below work direction:\n",
    "\n",
    "<ul>\n",
    "    <li> convert dtype of features where necessary eg from datetime to string or vice versa </li>\n",
    "    <li> drop rows with missing data or outliers </li>\n",
    "    <li> drop rows with duplicated data </li>\n",
    "    <li> group data for further data analysis </li>\n",
    "    <li> create new features with binaries from categorical variables </li>\n",
    "    <li> normalize data for columns with a different range of values </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning up the portfolio dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## copy a copy of the portfolio dataset\n",
    "portfolio_df  = portfolio.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new columns(web, emaiil,mobile & social) from channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_list  =  ['web', 'email', 'mobile', 'social'] \n",
    "\n",
    "for index in channels_list: \n",
    "    portfolio_df[index] = portfolio_df.channels.apply(lambda x: (index in x)+0)\n",
    "\n",
    "print(portfolio_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert offer_type from categorical into numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_df.offer_type = portfolio_df.offer_type.replace([ 'informational', 'bogo', 'discount'],\n",
    "                          [0, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the duration from day into hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_df.duration  = portfolio_df.duration * 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning up profile dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "changing the \"became_member_on\" to member since the earliest date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make a copy of profile\n",
    "profile_df = profile.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the anonmynous user with age 118, no gender and income information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_df = profile_df[profile_df.age!=118]\n",
    "profile_df_size = 17000-2175\n",
    "\n",
    "### sanity check on profile - after removing\n",
    "assert profile_df.shape[0] >= profile_df_size , \"the size is not right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the data in column \"became_member_on\" to \"membership\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rename the columns\n",
    "profile_df.rename(columns = {'became_member_on':'membership'}, inplace=True)\n",
    "                             \n",
    "base_year  = int(profile_df.membership.max() /10000) \n",
    "profile_df['membership']= profile_df.membership.apply(lambda x: base_year - int(x/10000))\n",
    "\n",
    "print(profile_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a master dataframe to hold all the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning up transcript dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_df = transcript.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing those records whose user does not appear in the new profile dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_df = transcript_df.value.apply(lambda x: (index in x)+0)\n",
    "\n",
    "transcript_df = transcript[transcript['person'].isin(profile_df['id'])]\n",
    "\n",
    "transcript_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have almost removed 10% of the records in transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_df.event.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the event categories into individual columns (binary) and then rename them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_list  =  ['transaction', 'offer received', 'offer viewed', 'offer completed'] \n",
    "\n",
    "transcript_df_events = pd.DataFrame()\n",
    "\n",
    "for index in event_list: \n",
    "    transcript_df_events[index] = transcript_df.event.apply(lambda x: 1 if x == index else 0 )\n",
    "\n",
    "### rename \n",
    "transcript_df_events.rename(columns={\"offer received\": \"offer_received\",\n",
    "                                     \"offer viewed\": \"offer_viewed\", \n",
    "                                     \"offer completed\": \"offer_completed\"}, inplace=True)\n",
    "\n",
    "### print out for view\n",
    "print(transcript_df_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### find out the events categories and number\n",
    "transcript_df_events.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the values categories into individual columns (binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_list  =  ['amount', 'offer_id', 'offer id'] \n",
    "\n",
    "transcript_df_value = pd.DataFrame()\n",
    "\n",
    "for index in value_list: \n",
    "    transcript_df_value[index] = transcript_df.value.apply(lambda x: x.get(index) if x.get(index) != None else \"\" )\n",
    "\n",
    "transcript_df_value['offer_id']   = transcript_df_value['offer_id']  + transcript_df_value['offer id'] \n",
    "transcript_df_value.drop('offer id', axis=1, inplace=True)\n",
    "print(transcript_df_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenating the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = [transcript_df, transcript_df_events,transcript_df_value]\n",
    "\n",
    "transcript_df = pd.concat(frame, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transcript_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would also need to filter out customers who completed the offer without recieving or viewing the offer to give a more balanced view. However first we would need to create a new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_df.offer_received.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we would want to create another DataFrame called offer which is based on every single offer made in the 10 campaign. The dataframe would include information from the 3 dataframe: \n",
    "<ul>\n",
    "    <li> portfolio\n",
    "    <li> profile\n",
    "    <li> transaction\n",
    "</ul>\n",
    "\n",
    "From transaction data frame, we would pull the following rows that have recieved the offer and the following columns\n",
    "<ul>\n",
    "    <li>person\n",
    "    <li>offer_id\n",
    "    <li>offer_viewed \n",
    "    <li>offer_completed\n",
    "    <li>offer_type\n",
    "    <li>amount\n",
    "</ul>\n",
    "\n",
    "From portfolio data frame, we would pull the following columns\n",
    "<ul>\n",
    "     <li>reward\n",
    "     <li>difficulty \n",
    "     <li>duration  \n",
    "     <li>web\n",
    "     <li>mobile\n",
    "     <li>social\n",
    "</ul>\n",
    "\n",
    "From profile data frame, we would pull the following columns\n",
    "<ul>\n",
    "     <li>gender\n",
    "     <li>age \n",
    "     <li>income  \n",
    "     <li>membership\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### we made a copy of whatever we did until now\n",
    "offer = transcript_df.copy()\n",
    "\n",
    "#### get all the offer that were send out\n",
    "offer = offer[offer.offer_received==1]\n",
    "\n",
    "#### drop all the unused column\n",
    "offer = offer.drop(['offer_received','event','value', 'time', 'transaction', 'amount'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the Python debugger module\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_df_clean  = transcript_df[transcript_df.transaction != 1]\n",
    "\n",
    "transcript_df_person_offerid = transcript_df_clean.groupby(['person', 'offer_id'])\n",
    "\n",
    "for key, item in transcript_df_person_offerid:\n",
    "    tmp = transcript_df_person_offerid.get_group(key)\n",
    "    print(tmp)\n",
    "    breakpoint()\n",
    "    if tmp.offer_completed.sum() == 1:\n",
    "        if tmp.offer_viewed.sum() == 1:\n",
    "            offer.loc[((offer.offer_id==key[1]) & (offer.person==key[0])), 'offer_completed'] = 1\n",
    "            offer.loc[((offer.offer_id==key[1]) & (offer.person==key[0])), 'offer_viewed'] = 1\n",
    "        else:\n",
    "            offer.loc[((offer.offer_id==key[1]) & (offer.person==key[0])), 'offer_completed'] = 0\n",
    "            offer.loc[((offer.offer_id==key[1]) & (offer.person==key[0])), 'offer_viewed'] = 0\n",
    "    \n",
    "    if tmp.offer_viewed.sum() == 1:\n",
    "        offer.loc[((offer.offer_id==key[1]) & (offer.person==key[0])), 'offer_viewed'] = 1\n",
    "\n",
    "print(offer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### joining the portfolio dataframe\n",
    "portfolio_df = portfolio_df.rename(columns={'id': 'offer_id'})\n",
    "\n",
    "portfolio_df.drop(['channels'], axis=1, inplace=True)\n",
    "\n",
    "### merge it\n",
    "offer = pd.merge(offer, portfolio_df, how='left')\n",
    "\n",
    "print(offer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### joining the profile dataframe\n",
    "profile_df = profile_df.rename(columns={'id': 'person'})\n",
    "\n",
    "### merge with offer\n",
    "offer = pd.merge(offer, profile_df, how='left')\n",
    "\n",
    "print(offer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is the age, gender and income demographics of the customer group? \n",
    "<ul>\n",
    "    <li>Who formed the majority of the Starbucks customer base ?\n",
    "    <li>How should we divide up the customer into grouping for later analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20, 14));\n",
    "\n",
    "### Length of Membership\n",
    "plt.subplot(2,2,1)\n",
    "plt.title(\"Distribution of Membership Duration\\n\",\n",
    "         fontweight =\"bold\")\n",
    "num_bins = 6\n",
    "n, bins, patches = plt.hist(offer['membership'], num_bins, \n",
    "                            color ='tab:blue',\n",
    "                            alpha = 0.8)\n",
    "\n",
    "plt.xlabel('Years of Being Member', fontweight =\"bold\")\n",
    "plt.ylabel('Customer', fontweight =\"bold\")\n",
    "\n",
    "\n",
    "### Age Distribution\n",
    "plt.subplot(2,2,2)\n",
    "plt.title('Histogram Plot of Customer Age Range\\n',\n",
    "          fontweight =\"bold\")\n",
    "num_bins = 12\n",
    "n, bins, patches = plt.hist(offer['age'], num_bins, \n",
    "                            color ='tab:blue',\n",
    "                            alpha = 0.8)\n",
    "\n",
    "plt.xlabel('Age', fontweight =\"bold\")\n",
    "plt.ylabel('Customers', fontweight =\"bold\")\n",
    "  \n",
    "### Income Distribution\n",
    "plt.subplot(2,2,3)\n",
    "plt.title(\"Income Distribution\\n\", fontweight =\"bold\")\n",
    "\n",
    "num_bins = 6\n",
    "n, bins, patches= plt.hist(offer['income'], num_bins, \n",
    "                            color ='tab:blue',\n",
    "                            alpha = 0.8)\n",
    "plt.xlabel('Income', fontweight =\"bold\")\n",
    "plt.ylabel('Customers', fontweight =\"bold\")\n",
    "\n",
    "### Gender Distribution\n",
    "plt.subplot(2,2,4)\n",
    "gender_m = offer[offer['gender'] == 'M'].gender.count()\n",
    "gender_f = offer[offer['gender'] == 'F'].gender.count()\n",
    "gender = [gender_m , gender_f ]\n",
    "labels=['Male', 'Female']\n",
    "\n",
    "plt.title(\"Gender Distribution\", fontweight =\"bold\")\n",
    "plt.pie(gender, labels=labels,  labeldistance=0.6, textprops={'fontsize': 18})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of gender, there is more men in the customer group than women. At the same time, it is interesting that most of the customer in the group seem to be around 40-70, peaking round 50+, a particularly mature customer group. It might be because the campaign intentionally target this group of mature customer, being naturally high power spending group(due to age). The mode of income is around 60000+\n",
    "\n",
    "Dividing the profile dataframe into 3 different income group, namely:\n",
    "<ol> \n",
    "    <li> from 53000 and below \n",
    "    <li> above 53000 and below 74000\n",
    "    <li> above 74000\n",
    "</ol>\n",
    "    \n",
    "Let us see the distribution of gender and age (which we shall use the quartile percentile age  as describe in the dataframe) within this group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20, 8))\n",
    "\n",
    "income_1  = offer.loc[offer.income <= 53000]\n",
    "income_2  = offer.loc[(offer.income > 53000) & (offer.income <= 73000)]\n",
    "income_3  = offer.loc[offer.income > 73000]\n",
    "\n",
    "### Gender within the group\n",
    "count_grp1   =  income_1.gender.count()\n",
    "count_grp2   =  income_2.gender.count()\n",
    "count_grp3   =  income_3.gender.count()\n",
    "\n",
    "m_count_grp1 =  income_1.loc[income_1.gender== 'M'].gender.count()\n",
    "f_count_grp1 =  income_1.loc[income_1.gender== 'F'].gender.count()\n",
    "m_count_grp2 =  income_2.loc[income_2.gender== 'M'].gender.count()\n",
    "f_count_grp2 =  income_2.loc[income_2.gender== 'F'].gender.count()\n",
    "m_count_grp3 =  income_3.loc[income_3.gender== 'M'].gender.count()\n",
    "f_count_grp3 =  income_3.loc[income_3.gender== 'F'].gender.count()\n",
    "\n",
    "income =[m_count_grp1, f_count_grp1, count_grp1,\n",
    "         m_count_grp2, f_count_grp2, count_grp2,\n",
    "         m_count_grp3, f_count_grp3, count_grp3]\n",
    "index  =['m_count_grp1', 'f_count_grp1', 'count_grp1',\n",
    "         'm_count_grp2', 'f_count_grp2', 'count_grp2',\n",
    "         'm_count_grp3', 'f_count_grp3', 'count_grp3']\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "\n",
    "color  = ['tab:blue', 'tab:pink', 'tab:green',\n",
    "          'tab:blue', 'tab:pink', 'tab:green',\n",
    "          'tab:blue', 'tab:pink', 'tab:green']\n",
    "\n",
    "plt.title(\"Distribution of Gender Among The Income Group\\n\",\n",
    "         fontweight =\"bold\")\n",
    "\n",
    "plt.ylabel(\"No of People\")\n",
    "plt.xlabel(\"Income group\")\n",
    "\n",
    "plt.bar(index, height=income, color = color)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "### Age within the group\n",
    "count_1Q_grp1   =  income_1.loc[income_1.age <= 45].age.count()\n",
    "count_2Q_grp1   =  income_1.loc[(income_1.age > 45) & (income_1.age <= 58)].age.count()\n",
    "count_3Q_grp1   =  income_1.loc[(income_1.age > 58) & (income_1.age <= 73)].age.count()\n",
    "count_4Q_grp1   =  income_1.loc[income_1.age >73].age.count()\n",
    "count_1Q_grp2   =  income_2.loc[income_2.age <= 45].age.count()\n",
    "count_2Q_grp2   =  income_2.loc[(income_2.age > 45) & (income_2.age <= 58)].age.count()\n",
    "count_3Q_grp2   =  income_2.loc[(income_2.age > 58) & (income_2.age <= 73)].age.count()\n",
    "count_4Q_grp2   =  income_2.loc[income_2.age >73].age.count()\n",
    "count_1Q_grp3   =  income_3.loc[income_3.age <= 45].age.count()\n",
    "count_2Q_grp3   =  income_3.loc[(income_3.age > 45) & (income_3.age <= 58)].age.count()\n",
    "count_3Q_grp3   =  income_3.loc[(income_3.age > 58) & (income_3.age <= 73)].age.count()\n",
    "count_4Q_grp3   =  income_3.loc[income_3.age >73].age.count()\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "\n",
    "color  = ['tab:orange', 'tab:purple', 'tab:olive', \"tab:cyan\",\n",
    "          'tab:orange', 'tab:purple', 'tab:olive', \"tab:cyan\",\n",
    "          'tab:orange', 'tab:purple', 'tab:olive', \"tab:cyan\"]\n",
    "income =[count_1Q_grp1, count_2Q_grp1, count_3Q_grp1, count_4Q_grp1, \n",
    "         count_1Q_grp2, count_2Q_grp2, count_3Q_grp2, count_4Q_grp2,\n",
    "         count_1Q_grp3, count_2Q_grp3, count_3Q_grp3, count_4Q_grp3]\n",
    "\n",
    "index  =['count_1Q_grp1', 'count_2Q_grp1','count_3Q_grp1', 'count_4Q_grp1',\n",
    "         'count_1Q_grp2', 'count_2Q_grp2', 'count_3Q_grp2', 'count_4Q_grp2',\n",
    "         'count_1Q_grp3', 'count_2Q_grp3', 'count_3Q_grp3', 'count_4Q_grp3',]\n",
    "\n",
    "plt.bar(index, height=income, color = color)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.title(\"Distribution of Customer Age Among The Income Group\\n\",\n",
    "         fontweight =\"bold\")\n",
    "\n",
    "plt.ylabel(\"No of People\")\n",
    "plt.xlabel(\"Customer Age group\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first talk about the age grouping within the income group. \n",
    "\n",
    "Observation\n",
    "<ul>\n",
    "    <li>As expected, the youngest of the group dominates the lower income (as probably they have not been in workforce for too long) and they almost twice as many when compared to the rest.\n",
    "    <li>The rest of the age-group exhibit increasing number of people when the income goes higher. \n",
    "    <li>There is almost no difference between the number of people in the 3 income group of age group 45-58 and 58-73.        \n",
    "</ul>\n",
    "\n",
    "For the income plot,  there is more males than female are found in the lower and mid-income level but the trend reverse for the high-level income group. This may be due to that at higher income group, it will require typically less manual and technical work which male could excel in but rather in sales and management jobs which females excel in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def is_equal_variance(d1, d2, alpha):\n",
    "    \"\"\"\n",
    "    Method for performing stats.levene test on two input samples. \n",
    "    As returns we get a boolian if sample variances are equal.\n",
    "\n",
    "    Args:\n",
    "        d1, d2 (pd.DataSeries): input sample data\n",
    "        alpha (float): alpha value as proportion \n",
    "\n",
    "    Returns:\n",
    "        boolian: true if variances between input distributions are equal (p > alpha), false otherwise (p <= alpha)\n",
    "        W: test statistic as float\n",
    "        p: p value for the test\n",
    "    \"\"\"\n",
    "    ### check if variances are equal   \n",
    "    W, p = stats.levene(d1, d2)\n",
    "    if p <= alpha:\n",
    "        print(f\"group variances unequal: W = {W:.4f}, p = {p:.4f}\")\n",
    "        return False        \n",
    "    else:\n",
    "        print(f\"group variances equal: W = {W:.4f}, p = {p:.4f}\")\n",
    "        return True\n",
    "\n",
    "def are_normal(d1, d2, alpha):\n",
    "    \"\"\"\n",
    "    Method for performing stats.normaltest on two input samples and returns boolian if sample distibutions are normal \n",
    "    As return we get a boolian if sample distibutions are normal.\n",
    "\n",
    "    Args:\n",
    "        d1, d2 (pd.DataSeries): input sample data\n",
    "        alpha (float): alpha value as proportion \n",
    "\n",
    "    Returns:\n",
    "        boolian: true if sample distributions are normal (p > alpha), false otherwise (p <= alpha)\n",
    "    \"\"\"\n",
    "    ### check if sample distributions are normal   \n",
    "    for d in [d1, d2]:\n",
    "        k2, p = stats.normaltest(d)\n",
    "        if p <= alpha:\n",
    "            print(f\"sample distribution not normal\")\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def t_test(d1, d2, alpha):\n",
    "    \"\"\"\n",
    "    Method for performing t-test on two sample distributions:\n",
    "        Welch's t-test for unequal variance\n",
    "        independent t-test for equal variance \n",
    "    For non-normal distributions of groups, sample size has to be of large size (appr. > 50) \n",
    "    to get a valid t-test on a non normal distribution.\n",
    "        \n",
    "    Args:\n",
    "        d1, d2 (pd.DataSeries): input sample data\n",
    "        alpha (float): alpha value as proportion \n",
    "    \n",
    "    Returns:\n",
    "        statistic (float): test statistic\n",
    "        p (float): p value for marginal significance\n",
    "        interpretation (string): interpretation of statistical significance for the two means\n",
    "    \"\"\"\n",
    "    ### check of equal variance\n",
    "    equal_var = is_equal_variance(d1, d2, alpha)\n",
    "    \n",
    "    ### check if group values are normally distributed\n",
    "    normal = are_normal(d1, d2, alpha)    \n",
    "    \n",
    "    ### select appropriate test\n",
    "    if not equal_var:\n",
    "        print(\"Welch's t-test on non-normal distributed samples with unequal variances:\")\n",
    "        statistic, p = stats.ttest_ind(d1, d2, equal_var = equal_var)\n",
    "    else:\n",
    "        print(\"Independent t-test:\")\n",
    "        statistic, p = stats.ttest_ind(d1, d2, equal_var = equal_var) \n",
    "        \n",
    "    ### t-test interpretation \n",
    "    significant = \"statistically significant\" if p <= alpha else \"not statistically significant\"\n",
    "    interpretation = f\"mean difference between groups is {significant}.\"\n",
    "    \n",
    "    return statistic, p, interpretation \n",
    "\n",
    "def compare_group_means(d1, d2, alpha):\n",
    "    \"\"\"\n",
    "    Method for performing sample means comparison with statistics summary of samples, \n",
    "    mean difference calulation, effect size test and t-test. \n",
    "    \n",
    "    Args:\n",
    "        d1, d2 (pd.DataSeries): input sample data\n",
    "        alpha (float): alpha value as proportion \n",
    "    \n",
    "    Returns: \n",
    "        print statements for statistics of samples, mean difference calulation, effect size test and t-test\n",
    "    \"\"\"\n",
    "    \n",
    "    ### aggregate statistics\n",
    "    print (\"sample statistics:\")\n",
    "    d1_describe = d1.agg([\"count\", \"mean\", \"median\", \"std\"])\n",
    "    d2_describe = d2.agg([\"count\", \"mean\", \"median\", \"std\"])\n",
    "    display(d1_describe, d2_describe)\n",
    "    \n",
    "    ### calculate difference between group means\n",
    "    diff = d1.mean() - d2.mean()\n",
    "    diffp = 100.0 * abs(diff) / d1.mean()\n",
    "    print(f\"mean difference between groups: {diff:.4f}({diffp:.2f}%)\\n\")\n",
    "    \n",
    "    ### Perform t-test\n",
    "    statistic, p, interpretation = t_test(d1, d2, alpha)\n",
    "    print(f\"statistic = {statistic:.4f} | p-value = {p:.4f} =>> {interpretation}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the 4 main client side factors namely (age, income, gender, length of membership)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### prepare data \n",
    "gender_1 = offer[offer[\"offer_completed\"] == 1][\"gender\"]\n",
    "gender_0 = offer[offer[\"offer_completed\"] == 0][\"gender\"]\n",
    "\n",
    "gender_1 = gender_1.apply(lambda x: 1 if x == \"F\" else 0)\n",
    "gender_0 = gender_0.apply(lambda x: 1 if x == \"F\" else 0)\n",
    "\n",
    "### print results of comparison between customer income with and without promotion success \n",
    "print(\"Gender - group means comparison between success and failure\")\n",
    "display(compare_group_means(gender_1, gender_0, alpha = 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### prepare data \n",
    "income_1 = offer[offer[\"offer_completed\"] == 1][\"income\"]\n",
    "income_0 = offer[offer[\"offer_completed\"] == 0][\"income\"]\n",
    "\n",
    "### print results of comparison between customer income with and without promotion success \n",
    "print(\"Income - group means comparison between success and failure\")\n",
    "display(compare_group_means(income_1, income_0, alpha = 0.05))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### prepare data series for statistical examination\n",
    "age_1 = offer[offer[\"offer_completed\"] == 1][\"income\"]\n",
    "age_0 = offer[offer[\"offer_completed\"] == 0][\"income\"]\n",
    "\n",
    "### print results of comparison between customer income with and without promotion success \n",
    "print(\"Age - group means comparison  between success and failure\")\n",
    "display(compare_group_means(age_1, age_0, alpha = 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = [18, 8])\n",
    "\n",
    "### left plot top: promotion success by customer's income\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(offer.loc[offer[\"offer_completed\"] == 1][\"income\"], \n",
    "             hist = False, color = \"orange\", kde_kws = {'shade': False})\n",
    "sns.distplot(offer.loc[offer[\"offer_completed\"] == 0][\"income\"], \n",
    "             hist = False, color = \"blue\", kde_kws = {'shade': False})\n",
    "\n",
    "plt.xlabel(\"income\")\n",
    "plt.title(\"Promotion success by Income Distribution\")\n",
    "plt.legend([\"success\", \"no success\"], frameon = False)\n",
    "plt.axvline(x = offer[offer[\"offer_completed\"] == 1][\"income\"].mean(), \n",
    "            color = \"orange\", linestyle = \"dashed\", linewidth = 1.5)\n",
    "plt.text(0.95 * offer.loc[offer[\"offer_completed\"] == 1][\"income\"].mean(), 0.00001, \n",
    "         f'mean (USD): {offer.loc[offer[\"offer_completed\"] == 1][\"income\"].mean():.0f}', \n",
    "         rotation = 90, verticalalignment = \"center\", color = \"orange\")\n",
    "plt.axvline(x = offer[offer[\"offer_completed\"] == 0][\"income\"].mean(), \n",
    "            color = \"blue\", linestyle = \"dashed\", linewidth = 1.5)\n",
    "plt.text(0.95 * offer.loc[offer[\"offer_completed\"] == 0][\"income\"].mean(), 0.00001, \n",
    "         f'mean (USD): {offer.loc[offer[\"offer_completed\"] == 0][\"income\"].mean():.0f}', \n",
    "         rotation = 90, verticalalignment = \"center\", color = \"blue\")\n",
    "\n",
    "### right plot top: promotion success by customer's age\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.distplot(offer.loc[offer[\"offer_completed\"] == 1][\"age\"], \n",
    "             hist = False, color = \"orange\", kde_kws = {'shade': False})\n",
    "sns.distplot(offer.loc[offer[\"offer_completed\"] == 0][\"age\"], \n",
    "             hist = False, color = \"blue\", kde_kws = {'shade': False})\n",
    "plt.xlabel(\"Age\")\n",
    "plt.title(\"Promotion success by Age Distribution\")\n",
    "plt.legend([\"success\", \"no success\"], frameon = False)\n",
    "plt.axvline(x = offer[offer[\"offer_completed\"] == 1][\"age\"].mean(), \n",
    "            color = \"orange\", linestyle = \"dashed\", linewidth = 1.5)\n",
    "plt.text(0.95 * offer.loc[offer[\"offer_completed\"] == 1][\"age\"].mean(), 0.015, \n",
    "         f'mean age: {offer.loc[offer[\"offer_completed\"] == 1][\"age\"].mean():.1f}', \n",
    "         rotation = 90, verticalalignment = \"center\", color = \"orange\")\n",
    "plt.axvline(x = offer[offer[\"offer_completed\"] == 0][\"age\"].mean(), \n",
    "            color = \"blue\", linestyle = \"dashed\", linewidth = 1.5)\n",
    "plt.text(0.95 * offer.loc[offer[\"offer_completed\"] == 0][\"age\"].mean(), 0.015, \n",
    "         f'mean age: {offer.loc[offer[\"offer_completed\"] == 0][\"age\"].mean():.1f}', \n",
    "         rotation = 90, verticalalignment = \"center\", color = \"blue\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = [18, 8])\n",
    "\n",
    "\n",
    "sns.distplot(gender_1, \n",
    "             hist = False, color = \"orange\", kde_kws = {'shade': False})\n",
    "sns.distplot(gender_0, \n",
    "             hist = False, color = \"blue\", kde_kws = {'shade': False})\n",
    "plt.xlabel(\"Gender\")\n",
    "plt.title(\"Promotion success by Gender Distributiom\")\n",
    "plt.legend([\"success\", \"no success\"], frameon = False)\n",
    "plt.axvline(x = gender_1.mean(), \n",
    "            color = \"orange\", linestyle = \"dashed\", linewidth = 1.5)\n",
    "plt.text(0.95 * gender_1.mean(), 0.015, \n",
    "         f'mean age: {gender_1.mean():.1f}', \n",
    "         rotation = 90, verticalalignment = \"center\", color = \"orange\")\n",
    "plt.axvline(x = gender_0.mean(), \n",
    "            color = \"blue\", linestyle = \"dashed\", linewidth = 1.5)\n",
    "plt.text(0.95 * gender_0, 0.015, \n",
    "         f'mean age: {gender_0.mean():.1f}', \n",
    "         rotation = 90, verticalalignment = \"center\", color = \"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Which is the most effective channel in dispensing out information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let us have an overview of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20, 12))\n",
    "\n",
    "n          = offer.offer_id.nunique()\n",
    "offer_list = offer.offer_id.unique()\n",
    "\n",
    "value = []\n",
    "for i in range(0,n):\n",
    "    value.append(1.0)\n",
    "    tmp  = offer[offer.offer_id==offer_list[i]]\n",
    "    \n",
    "    value.append(tmp.offer_viewed.sum() /tmp.shape[0])\n",
    "    value.append(tmp.offer_completed.sum() /tmp.shape[0])\n",
    "\n",
    "color = [] \n",
    "for i in range(0,n):\n",
    "    color.append(\"tab:olive\")\n",
    "    color.append(\"tab:orange\")\n",
    "    color.append(\"tab:cyan\")\n",
    "    \n",
    "index = []\n",
    "for i in range(0,n):\n",
    "    index.append(\"campaign_\" + str(i) + \"_offered\")\n",
    "    index.append(\"campaign_\" + str(i) + \"_viewed\")\n",
    "    index.append(\"campaign_\" + str(i) + \"_completed\")\n",
    "\n",
    "plt.bar(index, height=value, color = color)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.title(\"Distribution of Offer Viewing & Completion Among Various Offer\\n\",\n",
    "         fontweight =\"bold\")\n",
    "\n",
    "plt.ylabel(\"Ratio of Offer\")\n",
    "plt.xlabel(\"Various Offer & Responses\")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "success  = pd.DataFrame()\n",
    "success['offer_type'] = portfolio_df.offer_type\n",
    "success['difficulty'] = portfolio_df.difficulty\n",
    "success['reward']     = portfolio_df.reward\n",
    "success['duration']   = portfolio_df.duration\n",
    "success['completion'] = value[2::3]\n",
    "\n",
    "print(success)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Campaign 6,7,9, namely offer types: {discount, discount, bogo}, have above 60% completion rate. \n",
    "\n",
    "Observation\n",
    "<ul>\n",
    "    <li> At first glance, it seem that difficulty matters most in almost all types of offer. The more difficult it is, the lower is the completion rate,no matter the reward and time duration of the reward\n",
    "    <li> The type of offer seems to rank the next in the terms of effect, with discount offer being higher\n",
    "    <li> The influence of rewards seem to matter after next\n",
    "    <li> Duration seem to matter the least in all of this effect\n",
    "</ul>\n",
    "\n",
    "But is that so?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us go back to the question in which media channel offers the greatest attention. We would say media has the effect of bring attention to, ie in this case for customer to view the offer. Whether to complete it, would depend on the reward, difficulty of the offer and nature of customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(8, 4))\n",
    "\n",
    "influence_web    = (offer.offer_viewed * offer.web).sum() / ( offer.web).sum()  * 100\n",
    "influence_mobile = (offer.offer_viewed * offer.mobile).sum() / (offer.mobile).sum() * 100\n",
    "influence_social = (offer.offer_viewed * offer.social).sum() / (offer.social).sum() * 100\n",
    "\n",
    "influence = [influence_web, influence_mobile, influence_social] \n",
    "label = ['influence_web', 'influence_mobile', 'influence_social']\n",
    "\n",
    "color = ['tab:cyan', 'tab:orange', 'tab:olive']\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Plot of Viewing Rate (%) of Various Media Channel')\n",
    "\n",
    "plt.bar(label, influence, color=color)\n",
    "\n",
    "### Lastly to show\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprising, customer will respond more to their social media compared to other form of communication. This maybe because of social media give a more human touch and directed advertisement effect compared to mobile. Mobile, though personal, do not give one the personal touch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offer_m1 = offer.loc[ (offer.gender =='M') & (offer.age <= 45) ]\n",
    "offer_f1 = offer.loc[ (offer.gender =='F') & (offer.age <= 45) ]\n",
    "offer_m2 = offer.loc[(offer.gender =='M') & ( (offer.age > 45) & (offer.age <= 58))  ]\n",
    "offer_f2 = offer.loc[ (offer.gender =='F') & ( (offer.age > 45) & (offer.age <= 58))  ]\n",
    "offer_m3 = offer.loc[(offer.gender =='M') & ( (offer.age > 58) & (offer.age <= 73))]\n",
    "offer_f3 = offer.loc[ (offer.gender =='F') & ( (offer.age > 58) & (offer.age <= 73)) ]\n",
    "offer_m4 = offer.loc[(offer.gender =='M') & (offer.age > 73)]\n",
    "offer_f4 = offer.loc[ (offer.gender =='F') & (offer.age > 73) ]\n",
    "\n",
    "ratio_m1 = offer_m1.offer_viewed.sum() / offer_m1.shape[0]  * 100\n",
    "ratio_f1 = offer_f1.offer_viewed.sum() / offer_f1.shape[0]  * 100\n",
    "ratio_m2 = offer_m2.offer_viewed.sum() / offer_m2.shape[0]  * 100\n",
    "ratio_f2 = offer_f2.offer_viewed.sum() / offer_f2.shape[0]  * 100\n",
    "ratio_m3 = offer_m3.offer_viewed.sum() / offer_m3.shape[0]  * 100\n",
    "ratio_f3 = offer_f3.offer_viewed.sum() / offer_f3.shape[0]  * 100\n",
    "ratio_m4 = offer_m4.offer_viewed.sum() / offer_m4.shape[0]  * 100\n",
    "ratio_f4 = offer_f4.offer_viewed.sum() / offer_f4.shape[0]  * 100\n",
    "\n",
    "influence = [ratio_m1, ratio_f1, \n",
    "             ratio_m2, ratio_f2,\n",
    "             ratio_m3, ratio_f3, \n",
    "             ratio_m4, ratio_f4] \n",
    "label    =   ['ratio_m1', 'ratio_f1', \n",
    "             'ratio_m2', 'ratio_f2',\n",
    "             'ratio_m3', 'ratio_f3', \n",
    "             'ratio_m4', 'ratio_f4'] \n",
    "\n",
    "color = ['tab:cyan', 'tab:orange',\n",
    "        'tab:cyan', 'tab:orange',\n",
    "        'tab:cyan', 'tab:orange',\n",
    "        'tab:cyan', 'tab:orange']\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('\\n\\n Plot of Viewing Rate (%) of Various Media Channel ')\n",
    "\n",
    "plt.bar(label, influence, color=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of offers for each campaign is largely the same. So we can say, it is safe to compare between each "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seem to show that social media has great influence on the client. Most of the client actually responded to the campaign when it is hosted on social media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Which group (age, income, membership length) of customers appear to be more responsive towards the promotion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The customer response for Young Male is particularly poor. While the rest of the group exhibits at least 38% of offer uptake rate, Young Male group seem to have relatively 'poorer' uptake rate at about 28.2%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Is there link between the offer given and the target group?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Which type of promotional offer most appeal to which group of customer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Can we build a recommendation engine to recommend promotional offer with good uptake rate to new customers based on their demographical data age, income, registration date and gender? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selection of Features\n",
    "\n",
    "There is a plethora of features within the dataset we would need to fish out the most relevant features in our studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "### Plot Heatmap with feature multicollinearity\n",
    "sns.heatmap(offer.corr(), annot = True, vmin = -1, vmax = 1, fmt= \".2f\", cmap = \"coolwarm\")\n",
    "plt.title(\"Feature Multicollinearity - offer\", fontsize = \"x-large\")\n",
    "plt.xticks(rotation = 45, ha = \"right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metrics\n",
    "\n",
    "There are several metrics for evaluation the predictive performance of an algorithm. Here we will discuss the most common evaluation metrics used in the industry and which ones fit bests to our classification case.\n",
    "\n",
    "Our binary classification (with totally 2 classes) will produce a 2 x 2 confusion matrix with the prediction results on a test data set where we already know the true values. The output are 4 numbers: True Positive (Tp), True Negative (Tn), False Positive (Fp) and False Negative (Fn). The diagonal numbers (from top left to bottom right) represent the targets for which the predicted labels are equal to the test labels (Tp, Tn). The higher the count of the diagonal numbers the better the predictive performance [12].\n",
    "\n",
    "Evaluation metrics [12]:\n",
    "\n",
    "    Accuracy: is proportion of total number of predictions which were correct: (Tp + Tn)/(Tp + Fp + Fn + Tn). The output is a number in range [0, 1] where 1 is best and 0 is worst value. We can use the accuracy metric for classifications with roughly balanced output where our classes (success vs. no success) are roughly equal in size. For imbalanced output the validity of this metric dicreases.\n",
    "    Positive predictive value or Precision: is proportion of positive cases which were correctly classified: Tp/(Tp + Fp). The output is a number in range [0, 1] where 1 is best and 0 is worst value.\n",
    "    Negative predictive value: is proportion of negative cases which were correctly classified: Tn/(Tn + Fn). The output is a number in range [0, 1] where 1 is best and 0 is worst value.\n",
    "    Sensitivity or Recall: is proportion of actual positive cases which are correctly classified: Tp/(Tp + Fn). The output is a number in range [0, 1] where 1 is best and 0 is worst value.\n",
    "    Specificity: is proportion of actual negative cases which are correctly classified: Tn/(Tn + Fp). The output is a number in range [0, 1] where 1 is best and 0 is worst value.\n",
    "    F1-score: is harmonic mean of Precision and Recall: 2 * (precision * recall) / (precision + recall). The output is a number in range [0, 1] where 1 is best and 0 is worst value. The F1-score can be used for classifications with balanced and imbalanced output where false positive and false negative are equally important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(classifier, offer_id, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Method for training classifier and predicting on test set. \n",
    "    Output are: the trained classifier and the evaluation metrics accuracy and f1-score\n",
    "\n",
    "    Args:\n",
    "        classifier: classifier ith parameters, if procurable including GridSearchCV for parameter tuning\n",
    "        offer_id (int): integer with offer_id according master dataframe\n",
    "        X_train (pd.DataFrame): DataFrame with feature columns of train set\n",
    "        y_train (pd.DataSeries): Series with target column of train set\n",
    "        X_test (pd.DataFrame): DataFrame with feature columns of test set\n",
    "        y_test (pd.DataSeries): Series with target column of test set\n",
    "\n",
    "    Returns:\n",
    "        clf: trainied classifier\n",
    "        accuracy (float): accuracy score \n",
    "        f1 (float): f1-score\n",
    "        recall (float): recall score\n",
    "        precision (float): precision score\n",
    "    \"\"\"\n",
    "    ### train classifier more\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    ### predict on test set\n",
    "    pred = clf.predict(X_test)\n",
    "    ### accuracy score\n",
    "    accuracy = round(accuracy_score(y_test, pred)*100,2)\n",
    "    ### f1 score\n",
    "    f1 = round(f1_score(y_test, pred)*100,2)\n",
    "    ### recall score\n",
    "    recall = round(recall_score(y_test, pred)*100,2)\n",
    "    ### precision score\n",
    "    precision = round(precision_score(y_test, pred)*100,2)\n",
    "    \n",
    "    print(\"#######################################################\")\n",
    "    ### confusion matrix\n",
    "    cm = confusion_matrix(y_test, pred)\n",
    "    print(\"Offer {} - confusion matrix:\".format(offer_id))\n",
    "    print(cm, \"\\n\")\n",
    "    \n",
    "    ### classification report\n",
    "    cr = classification_report(y_test, pred, target_names = [\"0\", \"1\"])\n",
    "    print(\"Offer {} - classification report:\".format(offer_id))\n",
    "    print(cr)  \n",
    "    \n",
    "    print (\"Offer {}:\".format(offer_id),\n",
    "           \"Accuracy: {} % | F1-score: {} % \\n\\\n",
    "        Recall: {} % | Precision: {} %\".format(accuracy, f1, recall, precision),\"\\n\")\n",
    "    \n",
    "    return clf, accuracy, f1, recall, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "### define parameters and classifier\n",
    "parameters = {\"bootstrap\": [True],\n",
    "              \"max_depth\": [2, 6, 10], \n",
    "              \"max_features\": [0.5, 1, 2], \n",
    "              \"min_samples_leaf\": [1, 5], \n",
    "              \"min_samples_split\": [2, 5], \n",
    "              \"n_estimators\": [10, 20]}\n",
    "RF = RandomForestClassifier()\n",
    "clf = GridSearchCV(RF, parameters, scoring = \"roc_auc\", cv = 4, n_jobs = 4, verbose = 2)\n",
    "\n",
    "### train classifier, predict on test set and compute evaluation metrics\n",
    "rf_1, acc_rf_1, f1_rf_1, r_rf_1, p_rf_1 = classifier(clf, 1, X1_train, X1_test, y1_train, y1_test)\n",
    "rf_2, acc_rf_2, f1_rf_2, r_rf_2, p_rf_2 = classifier(clf, 2, X2_train, X2_test, y2_train, y2_test)\n",
    "rf_4, acc_rf_4, f1_rf_4, r_rf_4, p_rf_4 = classifier(clf, 4, X4_train, X4_test, y4_train, y4_test)\n",
    "rf_5, acc_rf_5, f1_rf_5, r_rf_5, p_rf_5 = classifier(clf, 5, X5_train, X5_test, y5_train, y5_test)\n",
    "rf_6, acc_rf_6, f1_rf_6, r_rf_6, p_rf_6 = classifier(clf, 6, X6_train, X6_test, y6_train, y6_test)\n",
    "rf_7, acc_rf_7, f1_rf_7, r_rf_7, p_rf_7 = classifier(clf, 7, X7_train, X7_test, y7_train, y7_test)\n",
    "rf_9, acc_rf_9, f1_rf_9, r_rf_9, p_rf_9 = classifier(clf, 9, X9_train, X9_test, y9_train, y9_test)\n",
    "rf_10, acc_rf_10, f1_rf_10, r_rf_10, p_rf_10 = classifier(clf, 10, X10_train, X10_test, y10_train, y10_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### add dummy columns for gender type categories. \n",
    "dummies_gender = pd.get_dummies(customer_data[\"gender\"], columns = [\"gender\"])\n",
    "customer_data = pd.concat([customer_data, dummies_gender.set_index(customer_data.index)], axis = 1)\n",
    "del customer_data[\"gender\"]\n",
    "\n",
    "### convert Dtype from object to datetime64\n",
    "customer_data[\"became_member_on\"] = pd.to_datetime(customer_data[\"became_member_on\"])\n",
    "### convert Dtype from datetime64 to ordinal\n",
    "customer_data[\"became_member_on\"] = customer_data[\"became_member_on\"].map(datetime.datetime.toordinal)\n",
    "\n",
    "### scale features of new customer data\n",
    "def scale_feature(ref_col, scale_col):\n",
    "    \"\"\"\n",
    "    Method for scaling one selected features column on min max values of a reference dataframe (i.e. master)\n",
    "\n",
    "    Args:\n",
    "        ref_df (pd.Series): Series with values whereof min and max values are taken for scaling\n",
    "        scale_df (pd.Series): Series with values of the same category as ref_df to be scaled\n",
    "\n",
    "    Returns:\n",
    "        scaled_values (pd.Series): Series with scaled values\n",
    "    \"\"\"\n",
    "    scaled_values = (scale_col - ref_col.min()) / (ref_col.max() - ref_col.min())\n",
    "\n",
    "    return scaled_values\n",
    "\n",
    "### scale columns age, become_member_on, income_year of new customer data\n",
    "customer_data[\"age\"] = scale_feature(master[\"age\"], customer_data[\"age\"])\n",
    "customer_data[\"became_member_on\"] = scale_feature(master[\"became_member_on\"], customer_data[\"became_member_on\"])\n",
    "customer_data[\"income year (USD)\"] = scale_feature(master[\"income year (USD)\"], customer_data[\"income year (USD)\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create dataframe with new customer data for prediction promomtion success\n",
    "customer_id = [\"customer_01\", \"customer_02\", \"customer_03\", \"customer_04\", \"customer_05\", \n",
    "               \"customer_06\", \"customer_07\", \"customer_08\", \"customer_09\", \"customer_10\"]\n",
    "age = [20, 25, 30, 35, 45, 50, 55, 60, 70, 80 ] # range: 18 - 101\n",
    "became_member_on = [\"2013-07-29\", \"2016-07-29\", \"2018-07-26\", \"2013-07-29\", \"2016-07-29\",\n",
    "                   \"2018-07-26\", \"2013-07-29\", \"2015-07-29\", \"2017-07-29\", \"2017-07-29\"] # range: 2013-07-29 - 2018-07-26\n",
    "income_year = [60000, 40000, 100000, 100000, 70000, 40000, 30000, 80000, 40000, 100000] # range: 30000 - 120000\n",
    "gender = [\"M\", \"F\", \"F\", \"F\", \"O\", \"M\", \"F\", \"F\", \"F\", \"M\"]\n",
    "\n",
    "customer_data = pd.DataFrame(\n",
    "    {\"age\": age,\n",
    "     \"became_member_on\": became_member_on,\n",
    "     \"income year (USD)\": income_year,\n",
    "     \"gender\": gender,\n",
    "     }, index = customer_id)\n",
    "display(customer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### predict promotion success for new customer data with selected trained classifier\n",
    "pred_offer_1 = rf_1.predict(customer_data)\n",
    "pred_offer_2 = knn_2.predict(customer_data)\n",
    "pred_offer_4 = rf_4.predict(customer_data)\n",
    "pred_offer_5 = dt_5.predict(customer_data)\n",
    "pred_offer_6 = svm_6.predict(customer_data)\n",
    "pred_offer_7 = gnb_7.predict(customer_data)\n",
    "pred_offer_9 = knn_9.predict(customer_data)\n",
    "pred_offer_10 = rf_10.predict(customer_data)\n",
    "\n",
    "### create dataframe\n",
    "pred_offer = pd.DataFrame(\n",
    "    {\"offer_01\": pred_offer_1,\n",
    "     \"offer_02\": pred_offer_2,\n",
    "     \"offer_04\": pred_offer_4,\n",
    "     \"offer_05\": pred_offer_5,\n",
    "     \"offer_06\": pred_offer_6,\n",
    "     \"offer_07\": pred_offer_7,\n",
    "     \"offer_09\": pred_offer_9,\n",
    "     \"offer_10\": pred_offer_10,\n",
    "     }, index = customer_id)\n",
    "\n",
    "pred_offer[\"sum success\"] = pred_offer[[\"offer_01\", \"offer_02\", \"offer_04\", \n",
    "                                        \"offer_05\", \"offer_06\", \"offer_07\", \n",
    "                                        \"offer_09\", \"offer_10\"]].sum(axis=1)\n",
    "\n",
    "display(\"Overview table with promotion success prediction for test customers\", pred_offer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# References \n",
    "\n",
    "<ol>\n",
    "    <li>Starbuck Wikipaedia, https://en.wikipedia.org/wiki/Starbucks</li>\n",
    "    <li>Starbuck Company Website, https://www.starbucks.com/about-us/ </li>\n",
    "    <li>https://digital.hbs.edu/platform-digit/submission/starbucks-mobile-app-a-winner-in-bridging-the-retail-digital-divide/ </li>\n",
    "    <li>List of the verified oldest people, <https://en.wikipedia.org/wiki/List_of_the_verified_oldest_people</li>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "cuda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
